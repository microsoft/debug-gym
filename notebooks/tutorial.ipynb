{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad290b0",
   "metadata": {},
   "source": [
    "# debug-gym: A Text-Based Environment for Interactive Debugging\n",
    "\n",
    "`debug-gym` is a text-based interactive debugging framework, designed for debugging Python programs.\n",
    "\n",
    "[[Technical Report](https://arxiv.org/abs/2503.21557)] [[Project Page](https://aka.ms/debug-gym/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d65b282",
   "metadata": {},
   "source": [
    "\n",
    "### Installation\n",
    "Follow the [installation instructions](https://github.com/microsoft/debug-gym#installation) to set up the environment. It should be as simple as running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4365541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv venv\n",
    "!source ./venv/bin/activate\n",
    "!uv pip install debug-gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f886f09b",
   "metadata": {},
   "source": [
    "### Tutorial\n",
    "\n",
    "`debug-gym` supports widely used coding benchmarks\n",
    "\n",
    "| Benchmark name | Link |\n",
    "| :-: | :----- |\n",
    "| `aider` | [https://github.com/Aider-AI/aider](https://github.com/Aider-AI/aider) |\n",
    "| `swebench`| [https://github.com/princeton-nlp/SWE-bench](https://github.com/princeton-nlp/SWE-bench) |\n",
    "| `swesmith`| [https://github.com/SWE-bench/SWE-smith](https://github.com/SWE-bench/SWE-smith) |\n",
    "| `mini_nightmare` | A set of 10 hand-crafted minimal buggy code snippet where rewrite only agents have harder time to tackle. Read details [here](https://github.com/microsoft/debug-gym/blob/main/data/mini_nightmare/mini_nightmare.md). |\n",
    "\n",
    "For this tutorial, we will use `mini_nightmare` benchmark, in particular the `pandas_dataframe` task, which is a buggy code snippet that requires the agent to inspect the columns of a pandas dataframe obtained from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3dc5737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tasks: ['config', 'counter', 'grader', 'pandas_dataframe', 'patcher', 'purr', 'scientific_calculator', 'shopping_cart', 'sum_tree', 'tomorrow_date']\n"
     ]
    }
   ],
   "source": [
    "from debug_gym.gym.envs import MiniNightmareEnv\n",
    "from debug_gym.logger import DebugGymLogger\n",
    "\n",
    "# For the sake of this tutorial, we disable the logger to avoid cluttering the output.\n",
    "logger = DebugGymLogger(\"debug-gym\", level=\"ERROR\")\n",
    "logger.disabled = True\n",
    "\n",
    "# Initialize the MiniNightmare benchmark environment.\n",
    "# This will download the dataset and set up the environment.\n",
    "env = MiniNightmareEnv(logger=logger)\n",
    "\n",
    "# Load the dataset and print the available tasks.\n",
    "print(f\"Available tasks: {sorted(env.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24af3075",
   "metadata": {},
   "source": [
    "#### Starting the `pandas_dataframe` task\n",
    "\n",
    "We will start the `pandas_dataframe` task using the `debug-gym` environment. This will initialize the environment and provide us with the first observation.\n",
    "\n",
    "`MiniNightmareEnv` is an interactive environment that follows the [Gymnasium](https://github.com/Farama-Foundation/Gymnasium) paradigm. Once the environment `env` is instantiated, one can use `env.reset()` to start an episode and receives initial informations. Then, one can interact with the environment using `env.step(action)`, where `action` specifies one of the available tools (see below), doing so will return subsequent informations (e.g, error message, debugger stdout, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc151dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                      DEBUG GYM ENVIRONMENT INFO                      \n",
      "======================================================================\n",
      "üìä Status: üîÑ (IN PROGRESS)\tüéØ Score: 0/1\t‚úèÔ∏è Rewrites: 0\n",
      "üëÅÔ∏è Observation:\n",
      "```\n",
      "env: ============================= test session starts ==============================\n",
      "collected 1 item\n",
      "\n",
      "test.py F\n",
      "\n",
      "=========================== short test summary info ============================\n",
      "FAILED test.py::test_calculate_stats - KeyError: 'Price'\n",
      "============================== 1 failed in 0.42s ===============================\n",
      "```\n",
      "\n",
      "üõ†Ô∏è  Available Tools (0):\n",
      "   \n",
      "\n",
      "üî¥ Breakpoints:\n",
      "   None set\n",
      "\n",
      "üìÅ Directory Structure:\n",
      "   Listing files in the current working directory. (read-only) indicates read-only files. Max depth: 1.\n",
      "   /tmp/RepoEnv-rvlu3vrs/\n",
      "   |-- pandas_dataframe_code.py\n",
      "   |-- test.py (read-only)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "info = env.reset(options={\"task_name\": \"pandas_dataframe\"})\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec1636",
   "metadata": {},
   "source": [
    "#### Adding tools\n",
    "\n",
    "One of the core designs of `debug-gym` is the notion of tools. Users can dynamically import tools, or develop customized tools and utilize them in the environment. Tools are modules that augment an agent's action space, observation space, or provide additonal functionalities to the agent. Below are the set of tools we have implemented so far.\n",
    "\n",
    "| Tool name | Description |\n",
    "| :-: | :----- |\n",
    "| `listdir` | It returns the directory tree at a given subdirectory. This is particularly useful when dealing with a repository with multiple files. |\n",
    "| `view` | It is used to change an agent's focus to a particular source code file. This is particularly useful when dealing with a repository with multiple files. |\n",
    "| `eval` | It runs the current code repository using the provided entrypoint (e.g., pytest), and returns the terminal's output (e.g., error message). |\n",
    "| `pdb` | Interactive debugger wrapping the [Python pdb tool](https://docs.python.org/3/library/pdb.html). In additon, users can choose to maintain a set of persistent breakpoints (as in some programming IDEs), which are not reset after every eval. With such feature, a new pdb debugging session is activated automatically, with all the breakpoints restored. Note such breakpoint can be cleared by pdb commands such as `cl`. |\n",
    "| `rewrite` | It can be used to rewrite a certain piece of code to fix the bug. The inputs of this tool call include the file path, the start and end line numbers, and the new code. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7413d136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                      DEBUG GYM ENVIRONMENT INFO                      \n",
      "======================================================================\n",
      "üìä Status: üîÑ (IN PROGRESS)\tüéØ Score: 0/1\t‚úèÔ∏è Rewrites: 0\n",
      "üëÅÔ∏è Observation:\n",
      "```\n",
      "Observation(source='env', observation=\"============================= test session starts ==============================\\ncollected 1 item\\n\\ntest.py F\\n\\n=========================== short test summary info ============================\\nFAILED test.py::test_calculate_stats - KeyError: 'Price'\\n============================== 1 failed in 0.60s ===============================\")\n",
      "```\n",
      "\n",
      "üõ†Ô∏è  Available Tools (3):\n",
      "   view, rewrite, eval\n",
      "\n",
      "üî¥ Breakpoints:\n",
      "   None set\n",
      "\n",
      "üìÅ Directory Structure:\n",
      "   Listing files in the current working directory. (read-only) indicates read-only files. Max depth: 1.\n",
      "   /tmp/RepoEnv-j8v36rxz/\n",
      "   |-- pandas_dataframe_code.py\n",
      "   |-- test.py (read-only)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from debug_gym.gym.tools.toolbox import Toolbox\n",
    "\n",
    "for tool in  [\"view\", \"rewrite\", \"eval\"]:\n",
    "    env.add_tool(Toolbox.get_tool(tool))\n",
    "\n",
    "info = env.reset(options={\"task_name\": \"pandas_dataframe\"})\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc107053",
   "metadata": {},
   "source": [
    "#### Setting up the LLM\n",
    "\n",
    "To use `debug-gym`, you need to set up a language model (LLM) that will be used as the agent to interact with the environment. You can use any LLM that is compatible with the `debug-gym` framework. Currently, we support OpenAI, Azure OpenAI, and Anthropic. We also support local LLMs via vLLM and using the OpenAI API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b7b80ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(id='gpt-4.1', created=1744316542, object='model', owned_by='system')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from debug_gym.llms.base import LLM, LLMConfig\n",
    "from debug_gym.llms import OpenAILLM\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model=\"gpt-4.1\",\n",
    "    context_limit=128,\n",
    "    endpoint=\"https://api.openai.com/v1\",\n",
    "    api_key=\"[OPENAI_API_KEY]\",\n",
    ")\n",
    "\n",
    "# llm = OpenAILLM(\"gpt-4.1\", logger=logger, llm_config=llm_config)\n",
    "llm = LLM.instantiate(\"gpt-4.1\", logger=logger)\n",
    "llm.client.models.retrieve(\"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b0c474",
   "metadata": {},
   "source": [
    "#### Running agent loop\n",
    "\n",
    "We provide the below LLM-based agents, they all have minimal design and serve the purpose of demonstrating the `debug-gym` APIs.\n",
    "\n",
    "| Agent name | Available Tools | Description |\n",
    "| :-: | :-: | :----- |\n",
    "| `debug_agent` | `pdb`, `rewrite`, `view`, `eval` | A minimal agent that dumps all available information into its prompt and queries the LLM to generate a command. |\n",
    "| `rewrite_agent` | `rewrite`, `view`, `eval`  | A `debug_agent` but `pdb` tool is disabled (an agent keeps rewriting). |\n",
    "| `debug_5_agent` | `pdb`, `rewrite`, `view`, `eval`  | A `debug_agent`, but `pdb` tool is only enabled after certain amount of rewrites. |\n",
    "| `solution_agent` | `pdb`, `eval`  | An oracle agent that applies a gold patch (only works with `swebench` and `swesmith` benchmarks for now). The agent checks that tests are failing before applying the patch, and passing after. It also checks that `pdb` tool can be used as expected. |\n",
    "\n",
    "For the sake of this tutorial, we will copy a minimal version of our agents running loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5645e9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools:\n",
      "- view(path, start, end, include_line_numbers_and_breakpoints): Specify a file path to view its content.\n",
      "- rewrite(path, start, end, new_code): Rewrite the content of the specified file path, between lines [start, end], with the new code.\n",
      "- eval(): Evaluate the current code against pre-defined test cases.\n",
      "- pdb(command): An interface to the Python debugger PDB.\n",
      "\n",
      "System prompt:\n",
      "\u001b[33mYour goal is to debug a Python program to make sure it can pass a set of test functions. You have access to a set of tools, you can use them to investigate the code and propose a rewriting patch to fix the bugs. Avoid rewriting the entire code, focus on the bugs only. At every step, you have to use one of the tools via function calling. You can only call one tool at a time. Do not repeat your previous action unless they can provide more information. You can think step by step to help you make the decision at every step, but you must be concise and avoid overthinking. Output both your thinking process (if any) and the tool call in the response.\u001b[0m\n",
      "\n",
      "==================== STEP 1 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35m============================= test session starts ==============================\n",
      "collected 1 item\n",
      "\n",
      "test.py F\n",
      "\n",
      "=========================== short test summary info ============================\n",
      "FAILED test.py::test_calculate_stats - KeyError: 'Price'\n",
      "============================== 1 failed in 0.40s ===============================\u001b[0m\n",
      "\u001b[36mThe error message indicates a KeyError for 'Price', which means that the code is trying to access a key named 'Price' in a dictionary or possibly a pandas DataFrame or Series, but that key doesn't exist. To debug this, I need to see where and how 'Price' is being used in the code.\n",
      "\n",
      "I will first look at the implementation in the main file (typically named something like main.py or a similarly key file), to find the code related to this error. Let's view the first 30 lines of main.py to search for the usage of 'Price'.\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_heEyJdsYdqBi1xBwqh3PvtC0', name='view', arguments={'path': 'main.py', 'start': 1, 'end': 30, 'include_line_numbers_and_breakpoints': True})\u001b[0m\n",
      "\n",
      "==================== STEP 2 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mView failed. Error message:\n",
      "`main.py` does not exist or is not in the working directory `/tmp/RepoEnv-7z9o_asz`.\u001b[0m\n",
      "\u001b[36mIt appears that there is no main.py file in the repository. Since the error comes from a test file named test.py, it's possible the main implementation is in a file referenced there, or the code is written in the test file itself.\n",
      "\n",
      "I will now view the first 30 lines of test.py to understand where the KeyError is coming from and which file or function is being tested.\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_s1RvB8e3JYIYJr0Mul17vheL', name='view', arguments={'path': 'test.py', 'start': 1, 'end': 30, 'include_line_numbers_and_breakpoints': True})\u001b[0m\n",
      "Step: 2 | Score: 0/1 (0.0%) | Reason: max steps reached\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from termcolor import colored\n",
    "\n",
    "from debug_gym.agents.history_tracker import HistoryTracker, build_history_prompt\n",
    "\n",
    "MAX_STEPS = 20  # Maximum number of steps to run the agent.\n",
    "MEMORY_SIZE = 20  # Size of the history tracker.\n",
    "TASK_NAME = \"pandas_dataframe\"\n",
    "SYSTEM_PROMPT = (\n",
    "    \"Your goal is to debug a Python program to make sure it can pass a set of test functions.\"\n",
    "    \" You have access to a set of tools, you can use them to investigate the code and propose a rewriting patch to fix the bugs.\"\n",
    "    \" Avoid rewriting the entire code, focus on the bugs only. At every step, you have to use one of the tools via function calling.\"\n",
    "    \" You can only call one tool at a time. Do not repeat your previous action unless they can provide more information.\"\n",
    "    \" You can think step by step to help you make the decision at every step, but you must be concise and avoid overthinking.\"\n",
    "    \" Output both your thinking process (if any) and the tool call in the response.\"\n",
    ")\n",
    "\n",
    "def run():\n",
    "    # We will use a history tracker to keep track of the agent's actions and observations.\n",
    "    # This will be used to build the prompt for the LLM.\n",
    "    history = HistoryTracker(MEMORY_SIZE)\n",
    "\n",
    "    # Let's reset the environment and get the initial state.\n",
    "    info = env.reset(options={\"task_name\": TASK_NAME})\n",
    "    history.step(new_info=info, llm_responses=None)  # initial state does not have response\n",
    "\n",
    "    # List the available tools that the LLM can use.\n",
    "    print(\"Available tools:\")\n",
    "    for tool in llm.define_tools(info.tools):\n",
    "        tool = tool.get(\"function\", {})\n",
    "        name = tool.get(\"name\", \"<unknown>\")\n",
    "        desc = tool.get(\"description\", \"\").split(\".\", 1)[0].strip() + \".\"\n",
    "        props = tool.get(\"parameters\", {}).get(\"properties\", {})\n",
    "        args = \", \".join(props.keys()) if props else \"\"\n",
    "        print(f\"- {name}({args}): {desc}\")\n",
    "\n",
    "    print(f\"\\nSystem prompt:\\n{colored(SYSTEM_PROMPT, 'yellow')}\")\n",
    "\n",
    "    highscore = info.score\n",
    "    for step in range(1, MAX_STEPS+1):\n",
    "        print(f\"\\n{'='*20} STEP {step} {'='*20}\")\n",
    "        highscore = max(highscore, info.score)\n",
    "        print(f\"Nb. of tests passed: {info.score:>4}/{info.max_score:<4} ({info.score/info.max_score:.1%}) [Best: {highscore}]\\n\")\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "        messages.extend(build_history_prompt(history, llm))\n",
    "\n",
    "        # Print the last observation from the environment.\n",
    "        print(colored(messages[-1][\"content\"], \"magenta\"))\n",
    "\n",
    "        # Send the prompt to the LLM and get the response.\n",
    "        llm_response = llm(messages, info.tools)\n",
    "\n",
    "        # Print the LLM response.\n",
    "        print(colored(f\"{llm_response.response}\", \"cyan\"))\n",
    "        print(colored(f\"Tool call: {llm_response.tool}\", \"cyan\", attrs=[\"bold\"]))\n",
    "\n",
    "        # Send the response to the environment and get the next state.\n",
    "        info = env.step(llm_response.tool, llm_response.response)\n",
    "        history.step(info, llm_response)\n",
    "\n",
    "        if info.done:\n",
    "            break\n",
    "\n",
    "    reason = \"bug fixed\" if info.done else \"max steps reached\"\n",
    "    print(f\"Step: {step} | Score: {info.score}/{info.max_score} ({info.score/info.max_score:.1%}) | Reason: {reason}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "history = run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec764ff",
   "metadata": {},
   "source": [
    "---\n",
    "Let's add the pdb tool to the environment and run the agent loop again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f198acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools:\n",
      "- view(path, start, end, include_line_numbers_and_breakpoints): Specify a file path to view its content.\n",
      "- rewrite(path, start, end, new_code): Rewrite the content of the specified file path, between lines [start, end], with the new code.\n",
      "- eval(): Evaluate the current code against pre-defined test cases.\n",
      "- pdb(command): An interface to the Python debugger PDB.\n",
      "\n",
      "System prompt:\n",
      "\u001b[33mYour goal is to debug a Python program to make sure it can pass a set of test functions. You have access to a set of tools, you can use them to investigate the code and propose a rewriting patch to fix the bugs. Avoid rewriting the entire code, focus on the bugs only. At every step, you have to use one of the tools via function calling. You can only call one tool at a time. Do not repeat your previous action unless they can provide more information. You can think step by step to help you make the decision at every step, but you must be concise and avoid overthinking. Output both your thinking process (if any) and the tool call in the response. You can use the `pdb` tool to set breakpoints and inspect variables.\u001b[0m\n",
      "\n",
      "==================== STEP 1 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35m============================= test session starts ==============================\n",
      "collected 1 item\n",
      "\n",
      "test.py F\n",
      "\n",
      "=========================== short test summary info ============================\n",
      "FAILED test.py::test_calculate_stats - KeyError: 'Price'\n",
      "============================== 1 failed in 0.57s ===============================\u001b[0m\n",
      "\u001b[36mThe test is failing due to a KeyError: 'Price', which usually means that the code is trying to access the column 'Price' in a DataFrame or dictionary, but that key doesn't exist. To debug this, I should inspect both the implementation and the test itself for how 'Price' is being used.\n",
      "\n",
      "Let's start by viewing the first 30 lines of `test.py` to see how the test is constructed and how the data is set up.\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_3CRGDk9irhyIs3NPSHo6dcDL', name='view', arguments={'path': 'test.py', 'start': 1, 'end': 30})\u001b[0m\n",
      "\n",
      "==================== STEP 2 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mViewing `test.py`, lines 1-8 of 8 total lines. The file is read-only.\n",
      "\n",
      "```\n",
      "     1 from pandas_dataframe_code import calculate_stats, load_data\n",
      "     2 \n",
      "     3 \n",
      "     4 def test_calculate_stats():\n",
      "     5     df = load_data()\n",
      "     6     median = calculate_stats(df)\n",
      "     7 \n",
      "     8     assert median == 14.4542\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[36mNone\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_SjApBRM0tqMDkpA0J5age4tZ', name='view', arguments={'path': 'pandas_dataframe_code.py', 'start': 1, 'end': 30})\u001b[0m\n",
      "\n",
      "==================== STEP 3 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mViewing `pandas_dataframe_code.py`, lines 1-17 of 17 total lines.\n",
      "\n",
      "```\n",
      "     1 import pandas as pd\n",
      "     2 \n",
      "     3 \n",
      "     4 def load_data():\n",
      "     5     # URL of the Titanic dataset (CSV format)\n",
      "     6     url = \"https://gist.githubusercontent.com/chisingh/d004edf19fffe92331e153a39466d38c/raw/titanic.csv\"\n",
      "     7 \n",
      "     8     # Read the CSV file\n",
      "     9     df = pd.read_csv(url)\n",
      "    10 \n",
      "    11     return df\n",
      "    12 \n",
      "    13 def calculate_stats(df):\n",
      "    14     # Calculate the median fare paid\n",
      "    15     median = df['Price'].median()\n",
      "    16     print(\"\\nMedian fare paid:\\n\", median)\n",
      "    17     return median\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[36mFrom the code, in `calculate_stats(df)`, the function tries to get the median of `df['Price']`. However, standard Titanic datasets use the column name `'Fare'` instead of `'Price'`. This is likely the cause of the KeyError.\n",
      "\n",
      "I will now prepare a patch to change `df['Price']` to `df['Fare']` in the `calculate_stats` function.\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_B4JstcUGqepFe5NAoCAzDQWA', name='rewrite', arguments={'path': 'pandas_dataframe_code.py', 'start': 15, 'end': 15, 'new_code': \"    median = df['Fare'].median()\"})\u001b[0m\n",
      "\n",
      "==================== STEP 4 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mThe file `pandas_dataframe_code.py` has been updated successfully.\n",
      "\n",
      "Diff:\n",
      "\n",
      "--- original\n",
      "+++ current\n",
      "@@ -12,6 +12,6 @@\n",
      " \n",
      " def calculate_stats(df):\n",
      "     # Calculate the median fare paid\n",
      "-    median = df['Price'].median()\n",
      "+    median = df['Fare'].median()\n",
      "     print(\"\\nMedian fare paid:\\n\", median)\n",
      "     return median\n",
      "\u001b[0m\n",
      "\u001b[36mNone\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_yX9qPJyVP1P3Cu1R9kHMUrbe', name='eval', arguments={})\u001b[0m\n",
      "\n",
      "==================== STEP 5 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35m============================= test session starts ==============================\n",
      "collected 1 item\n",
      "\n",
      "test.py F\n",
      "\n",
      "=========================== short test summary info ============================\n",
      "FAILED test.py::test_calculate_stats - KeyError: 'Fare'\n",
      "============================== 1 failed in 0.39s ===============================\u001b[0m\n",
      "\u001b[36mThe error has shifted to a KeyError: 'Fare', which means even 'Fare' is not in the dataframe. This suggests that the dataset loaded from the provided URL might use a different column name for the fare/price or isn't loading as expected.\n",
      "\n",
      "I need to inspect the first few lines or columns of the dataframe returned by `load_data()` to see what columns are available. I'll print (or inspect in pdb) the columns of the dataframe after loading. Let's set a breakpoint at line 11 (right after returning the df in `load_data`) and use pdb to inspect the columns.\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_dQpC9vON9yTEXPHDS4Vslr7l', name='pdb', arguments={'command': 'b pandas_dataframe_code.py:11'})\u001b[0m\n",
      "\n",
      "==================== STEP 6 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mPdb command output:\n",
      "Breakpoint 1 at /tmp/RepoEnv-q5dccgwr/pandas_dataframe_code.py:11\n",
      "\n",
      "Current frame:\n",
      "/home/macote/src/debug-gym/.venv/lib/python3.12/site-packages/pytest/__main__.py\n",
      "\n",
      "Context around the current frame:\n",
      "  1  ->\t\"\"\"The pytest entry point.\"\"\"\n",
      "  2  \t\n",
      "  3  \tfrom __future__ import annotations\n",
      "  4  \t\n",
      "  5  \timport pytest\n",
      "  6  \t\n",
      "  7  \t\n",
      "  8  \tif __name__ == \"__main__\":\n",
      "  9  \t    raise SystemExit(pytest.console_main())\n",
      "[EOF]\n",
      "\u001b[0m\n",
      "\u001b[36mNone\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_RGfqCGZokwwZDAVirNh4PK2x', name='pdb', arguments={'command': 'c'})\u001b[0m\n",
      "\n",
      "==================== STEP 7 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mPdb command output:\n",
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.12.3, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /tmp/RepoEnv-q5dccgwr\n",
      "plugins: anyio-4.10.0\n",
      "collected 1 item                                                               \n",
      "\n",
      "test.py > /tmp/RepoEnv-q5dccgwr/pandas_dataframe_code.py(11)load_data()\n",
      "-> return df\n",
      "\n",
      "Current frame:\n",
      "/tmp/RepoEnv-q5dccgwr/pandas_dataframe_code.py\n",
      "\n",
      "Context around the current frame:\n",
      "  6  \t    url = \"https://gist.githubusercontent.com/chisingh/d004edf19fffe92331e153a39466d38c/raw/titanic.csv\"\n",
      "  7  \t\n",
      "  8  \t    # Read the CSV file\n",
      "  9  \t    df = pd.read_csv(url)\n",
      " 10  \t\n",
      " 11 B->\t    return df\n",
      " 12  \t\n",
      " 13  \tdef calculate_stats(df):\n",
      " 14  \t    # Calculate the median fare paid\n",
      " 15  \t    median = df['Fare'].median()\n",
      " 16  \t    print(\"\\nMedian fare paid:\\n\", median)\n",
      "\u001b[0m\n",
      "\u001b[36mNone\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_4yArxd3qBgTSCy6ctIhQLjpE', name='pdb', arguments={'command': 'p df.columns'})\u001b[0m\n",
      "\n",
      "==================== STEP 8 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mPdb command output:\n",
      "Index(['pclass', 'survived', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket',\n",
      "       'prix', 'cabin', 'embarked', 'boat', 'body', 'home.dest'],\n",
      "      dtype='object')\n",
      "\n",
      "Current frame:\n",
      "/tmp/RepoEnv-q5dccgwr/pandas_dataframe_code.py\n",
      "\n",
      "Context around the current frame:\n",
      "  6  \t    url = \"https://gist.githubusercontent.com/chisingh/d004edf19fffe92331e153a39466d38c/raw/titanic.csv\"\n",
      "  7  \t\n",
      "  8  \t    # Read the CSV file\n",
      "  9  \t    df = pd.read_csv(url)\n",
      " 10  \t\n",
      " 11 B->\t    return df\n",
      " 12  \t\n",
      " 13  \tdef calculate_stats(df):\n",
      " 14  \t    # Calculate the median fare paid\n",
      " 15  \t    median = df['Fare'].median()\n",
      " 16  \t    print(\"\\nMedian fare paid:\\n\", median)\n",
      "\u001b[0m\n",
      "\u001b[36mThe column containing the fare is actually named `'prix'` (French for 'price') in this dataset, not 'Fare' or 'Price'.\n",
      "\n",
      "I will fix the code in `calculate_stats` to use `df['prix']` instead.\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_QquGq06cJzSo13O9skufhwAQ', name='rewrite', arguments={'path': 'pandas_dataframe_code.py', 'start': 15, 'end': 15, 'new_code': \"    median = df['prix'].median()\"})\u001b[0m\n",
      "Step: 8 | Score: 1/1 (100.0%) | Reason: bug fixed\n"
     ]
    }
   ],
   "source": [
    "if not env.has_tool(\"pdb\"):\n",
    "    env.add_tool(Toolbox.get_tool(\"pdb\"))\n",
    "    SYSTEM_PROMPT += (\n",
    "        \" You can use the `pdb` tool to set breakpoints and inspect variables.\"\n",
    "    )\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6284c",
   "metadata": {},
   "source": [
    "#### Analysis and Visualization\n",
    "\n",
    "We provide a script, `json_log_viewer.py`, to help analyze the log files (e.g., the `.jsonl` files) generated by the `debug-gym` agents.\n",
    "\n",
    "First let's save the agent's history to a `.jsonl` file. This file will contain all the interactions between the agent and the environment, including the actions taken by the agent, the observations received, and any errors encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88634798",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_output = {\n",
    "    \"problem\": TASK_NAME,\n",
    "    \"config\": {},\n",
    "    \"tools\": llm.define_tools(env.tools),\n",
    "    \"uuid\": \"N/A\",\n",
    "    \"success\": env.done,\n",
    "    \"log\": [history.json(i) for i in range(len(history))],\n",
    "    \"agent_type\": \"custom\",\n",
    "    \"logger\": \"N/A\",\n",
    "}\n",
    "\n",
    "with open(\"tutorial.jsonl\", \"w\") as f:\n",
    "    json.dump(jsonl_output, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e715fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the viewer.\n",
    "!python ../analysis/json_log_viewer/json_log_viewer.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "debug-gym (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
