{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad290b0",
   "metadata": {},
   "source": [
    "# debug-gym: A Text-Based Environment for Interactive Debugging\n",
    "\n",
    "`debug-gym` is a text-based interactive debugging framework, designed for debugging Python programs.\n",
    "\n",
    "[[Technical Report](https://arxiv.org/abs/2503.21557)] [[Project Page](https://aka.ms/debug-gym/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d65b282",
   "metadata": {},
   "source": [
    "\n",
    "### Installation\n",
    "Follow the [installation instructions](https://github.com/microsoft/debug-gym#installation) to set up the environment. It should be as simple as running:\n",
    "\n",
    "```bash\n",
    "# Clone the repository and set up the virtual environment\n",
    "git clone https://github.com/microsoft/debug-gym.git -b tutorial\n",
    "cd debug-gym\n",
    "\n",
    "# Install uv and create a virtual environment\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "uv venv\n",
    "source ./venv/bin/activate\n",
    "\n",
    "# Install the package in editable mode\n",
    "uv pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In codespace you can simply install the package directly from the notebook.\n",
    "!pip install -e .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f886f09b",
   "metadata": {},
   "source": [
    "### Tutorial\n",
    "\n",
    "`debug-gym` supports widely used coding benchmarks\n",
    "\n",
    "| Benchmark name | Link |\n",
    "| :-: | :----- |\n",
    "| `aider` | [https://github.com/Aider-AI/aider](https://github.com/Aider-AI/aider) |\n",
    "| `swebench`| [https://github.com/princeton-nlp/SWE-bench](https://github.com/princeton-nlp/SWE-bench) |\n",
    "| `swesmith`| [https://github.com/SWE-bench/SWE-smith](https://github.com/SWE-bench/SWE-smith) |\n",
    "| `mini_nightmare` | A set of 10 hand-crafted minimal buggy code snippet where rewrite only agents have harder time to tackle. Read details [here](https://github.com/microsoft/debug-gym/blob/main/data/mini_nightmare/mini_nightmare.md). |\n",
    "\n",
    "\n",
    "For this tutorial, we will use `mini_nightmare` benchmark, in particular the `pandas_dataframe` task, which is a buggy code snippet that requires the agent to inspect the columns of a pandas dataframe obtained from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3dc5737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tasks: ['config', 'counter', 'grader', 'pandas_dataframe', 'patcher', 'purr', 'scientific_calculator', 'shopping_cart', 'sum_tree', 'tomorrow_date']\n"
     ]
    }
   ],
   "source": [
    "from debug_gym.gym.envs import MiniNightmareEnv\n",
    "from debug_gym.logger import DebugGymLogger\n",
    "\n",
    "# For the sake of this tutorial, we disable the logger to avoid cluttering the output.\n",
    "logger = DebugGymLogger(\"debug-gym\", level=\"ERROR\")\n",
    "logger.disabled = True\n",
    "\n",
    "# Initialize the MiniNightmare benchmark environment.\n",
    "# This will download the dataset and set up the environment.\n",
    "env = MiniNightmareEnv(logger=logger)\n",
    "\n",
    "# Load the dataset and print the available tasks.\n",
    "print(f\"Available tasks: {sorted(env.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24af3075",
   "metadata": {},
   "source": [
    "#### Starting the `pandas_dataframe` task\n",
    "\n",
    "We will start the `pandas_dataframe` task using the `debug-gym` environment. This will initialize the environment and provide us with the first observation.\n",
    "\n",
    "`MiniNightmareEnv` is an interactive environment that follows the [Gymnasium](https://github.com/Farama-Foundation/Gymnasium) paradigm. Once the environment `env` is instantiated, one can use `env.reset()` to start an episode and receives initial informations. Then, one can interact with the environment using `env.step(action)`, where `action` specifies one of the available tools (see below), doing so will return subsequent informations (e.g, error message, debugger stdout, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc151dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                      DEBUG GYM ENVIRONMENT INFO                      \n",
      "======================================================================\n",
      "ðŸ“Š Status: ðŸ”„ (IN PROGRESS)\tðŸŽ¯ Score: 0/1\tâœï¸ Rewrites: 0\n",
      "ðŸ‘ï¸ Observation:\n",
      "```\n",
      "============================= test session starts ==============================\n",
      "collected 1 item\n",
      "\n",
      "test.py F\n",
      "\n",
      "=========================== short test summary info ============================\n",
      "FAILED test.py::test_calculate_stats - KeyError: 'Price'\n",
      "============================== 1 failed in 0.39s ===============================\n",
      "```\n",
      "\n",
      "ðŸ› ï¸  Available Tools (0):\n",
      "   \n",
      "\n",
      "ðŸ”´ Breakpoints:\n",
      "   None set\n",
      "\n",
      "ðŸ“ Directory Structure:\n",
      "   Listing files in the current working directory. (read-only) indicates read-only files. Max depth: 1.\n",
      "   /tmp/RepoEnv-hk6i04hj/\n",
      "   |-- pandas_dataframe_code.py\n",
      "   |-- test.py (read-only)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "info = env.reset(options={\"task_name\": \"pandas_dataframe\"})\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec1636",
   "metadata": {},
   "source": [
    "#### Adding tools ðŸ› ï¸\n",
    "\n",
    "One of the core designs of `debug-gym` is the notion of tools. Users can dynamically import tools, or develop customized tools and utilize them in the environment. Tools are modules that augment an agent's action space, observation space, or provide additonal functionalities to the agent. Below are the set of tools we have implemented so far.\n",
    "\n",
    "| Tool name | Description |\n",
    "| :-: | :----- |\n",
    "| `listdir` | It returns the directory tree at a given subdirectory. This is particularly useful when dealing with a repository with multiple files. |\n",
    "| `view` | It is used to change an agent's focus to a particular source code file. This is particularly useful when dealing with a repository with multiple files. |\n",
    "| `eval` | It runs the current code repository using the provided entrypoint (e.g., pytest), and returns the terminal's output (e.g., error message). |\n",
    "| `pdb` | Interactive debugger wrapping the [Python pdb tool](https://docs.python.org/3/library/pdb.html). In additon, users can choose to maintain a set of persistent breakpoints (as in some programming IDEs), which are not reset after every eval. With such feature, a new pdb debugging session is activated automatically, with all the breakpoints restored. Note such breakpoint can be cleared by pdb commands such as `cl`. |\n",
    "| `rewrite` | It can be used to rewrite a certain piece of code to fix the bug. The inputs of this tool call include the file path, the start and end line numbers, and the new code. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7413d136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                      DEBUG GYM ENVIRONMENT INFO                      \n",
      "======================================================================\n",
      "ðŸ“Š Status: ðŸ”„ (IN PROGRESS)\tðŸŽ¯ Score: 0/1\tâœï¸ Rewrites: 0\n",
      "ðŸ‘ï¸ Observation:\n",
      "```\n",
      "============================= test session starts ==============================\n",
      "collected 1 item\n",
      "\n",
      "test.py F\n",
      "\n",
      "=========================== short test summary info ============================\n",
      "FAILED test.py::test_calculate_stats - KeyError: 'Price'\n",
      "============================== 1 failed in 0.39s ===============================\n",
      "```\n",
      "\n",
      "ðŸ› ï¸  Available Tools (3):\n",
      "   view(path:string, start:number, end:number, include_line_numbers_and_breakpoints:boolean): Specify a file path to view its content.\n",
      "   rewrite(path:string, start:number, end:number, new_code:string): Rewrite the content of the specified file path, between lines [start, end], with the new code.\n",
      "   eval(): Evaluate the current code against pre-defined test cases.\n",
      "\n",
      "ðŸ”´ Breakpoints:\n",
      "   None set\n",
      "\n",
      "ðŸ“ Directory Structure:\n",
      "   Listing files in the current working directory. (read-only) indicates read-only files. Max depth: 1.\n",
      "   /tmp/RepoEnv-32lac0qw/\n",
      "   |-- pandas_dataframe_code.py\n",
      "   |-- test.py (read-only)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from debug_gym.gym.tools.toolbox import Toolbox\n",
    "\n",
    "for tool in  [\"view\", \"rewrite\", \"eval\"]:\n",
    "    env.add_tool(Toolbox.get_tool(tool))\n",
    "\n",
    "info = env.reset(options={\"task_name\": \"pandas_dataframe\"})\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc107053",
   "metadata": {},
   "source": [
    "#### Setting up the LLM\n",
    "\n",
    "To use `debug-gym`, you need to set up a language model (LLM) that will be used as the agent to interact with the environment. You can use any LLM that is compatible with the `debug-gym` framework. Currently, we support OpenAI, Azure OpenAI, and Anthropic. We also support local LLMs via vLLM and using the OpenAI API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b7b80ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(id='gpt-4.1', created=1744316542, object='model', owned_by='system')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from debug_gym.llms.base import LLM, LLMConfig\n",
    "from debug_gym.llms import OpenAILLM\n",
    "\n",
    "MODEL = \"gpt-4.1\"  # gpt-5 tool calling seems broken at the moment ðŸ¤·.\n",
    "llm_config = LLMConfig(\n",
    "    model=MODEL,\n",
    "    context_limit=128,\n",
    "    endpoint=\"https://api.openai.com/v1\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    ")\n",
    "\n",
    "llm = OpenAILLM(MODEL, logger=logger, llm_config=llm_config)\n",
    "llm.client.models.retrieve(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b0c474",
   "metadata": {},
   "source": [
    "#### Running agent loop\n",
    "\n",
    "We provide the below LLM-based agents, they all have minimal design and serve the purpose of demonstrating the `debug-gym` APIs.\n",
    "\n",
    "| Agent name | Available Tools | Description |\n",
    "| :-: | :-: | :----- |\n",
    "| `debug_agent` | `pdb`, `rewrite`, `view`, `eval` | A minimal agent that dumps all available information into its prompt and queries the LLM to generate a command. |\n",
    "| `rewrite_agent` | `rewrite`, `view`, `eval`  | A `debug_agent` but `pdb` tool is disabled (an agent keeps rewriting). |\n",
    "| `debug_5_agent` | `pdb`, `rewrite`, `view`, `eval`  | A `debug_agent`, but `pdb` tool is only enabled after certain amount of rewrites. |\n",
    "| `solution_agent` | `pdb`, `eval`  | An oracle agent that applies a gold patch (only works with `swebench` and `swesmith` benchmarks for now). The agent checks that tests are failing before applying the patch, and passing after. It also checks that `pdb` tool can be used as expected. |\n",
    "\n",
    "For the sake of this tutorial, we will copy a minimal version of our agents running loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5645e9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools:\n",
      "- view(path, start, end, include_line_numbers_and_breakpoints): Specify a file path to view its content.\n",
      "- rewrite(path, start, end, new_code): Rewrite the content of the specified file path, between lines [start, end], with the new code.\n",
      "- eval(): Evaluate the current code against pre-defined test cases.\n",
      "\n",
      "System prompt:\n",
      "\u001b[33mYour goal is to debug a Python program to make sure it can pass a set of test functions. You have access to a set of tools, you can use them to investigate the code and propose a rewriting patch to fix the bugs. Avoid rewriting the entire code, focus on the bugs only. At every step, you have to use one of the tools via function calling. You can only call one tool at a time. Do not repeat your previous action unless they can provide more information. You can think step by step to help you make the decision at every step, but you must be concise and avoid overthinking. Output both your thinking process (if any) and the tool call in the response.\u001b[0m\n",
      "\n",
      "==================== STEP 1 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35m============================= test session starts ==============================\n",
      "collected 1 item\n",
      "\n",
      "test.py F\n",
      "\n",
      "=========================== short test summary info ============================\n",
      "FAILED test.py::test_calculate_stats - KeyError: 'Price'\n",
      "============================== 1 failed in 0.41s ===============================\u001b[0m\n",
      "\u001b[36mThe test failed with a KeyError for 'Price', which suggests the code attempted to access a dictionary key or dataframe column called 'Price' that doesn't exist. Let's check the code in the relevant file where this key is accessed. Assuming it's in the main implementation file, let's view the first 30 lines of main.py to investigate.\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_eRR0MxIb67pfvFRz0o2qemN6', name='view', arguments={'path': 'main.py', 'start': 1, 'end': 30, 'include_line_numbers_and_breakpoints': True})\u001b[0m\n",
      "\n",
      "==================== STEP 2 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mView failed. Error message:\n",
      "`main.py` does not exist or is not in the working directory `/tmp/RepoEnv-wmbm3sgj`.\u001b[0m\n",
      "\u001b[36mThere's no main.py file. Since the error is about a KeyError: 'Price', let's check the contents of test.py instead. The test might show which file or function is being tested and what is expected, guiding us to the file to inspect.\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_meMCEf3wtoMOP1dIE1qN0lej', name='view', arguments={'path': 'test.py', 'start': 1, 'end': 30, 'include_line_numbers_and_breakpoints': True})\u001b[0m\n",
      "\n",
      "==================== STEP 3 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mViewing `test.py`, lines 1-8 of 8 total lines. The file is read-only.\n",
      "\n",
      "```\n",
      "     1 from pandas_dataframe_code import calculate_stats, load_data\n",
      "     2 \n",
      "     3 \n",
      "     4 def test_calculate_stats():\n",
      "     5     df = load_data()\n",
      "     6     median = calculate_stats(df)\n",
      "     7 \n",
      "     8     assert median == 14.4542\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[36mThe functions being tested are `calculate_stats` and `load_data` from `pandas_dataframe_code`. Since the error was a KeyError for 'Price' and the test involves a DataFrame, we should inspect `pandas_dataframe_code.py` to see how these functions handle DataFrame columns and where 'Price' is referenced.\n",
      "\n",
      "Let's view the first 30 lines of `pandas_dataframe_code.py`.\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_YBDpxvk7tMeuHQ0hKRmjRCu9', name='view', arguments={'path': 'pandas_dataframe_code.py', 'start': 1, 'end': 30, 'include_line_numbers_and_breakpoints': True})\u001b[0m\n",
      "\n",
      "==================== STEP 4 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mViewing `pandas_dataframe_code.py`, lines 1-17 of 17 total lines.\n",
      "\n",
      "```\n",
      "     1 import pandas as pd\n",
      "     2 \n",
      "     3 \n",
      "     4 def load_data():\n",
      "     5     # URL of the Titanic dataset (CSV format)\n",
      "     6     url = \"https://gist.githubusercontent.com/chisingh/d004edf19fffe92331e153a39466d38c/raw/titanic.csv\"\n",
      "     7 \n",
      "     8     # Read the CSV file\n",
      "     9     df = pd.read_csv(url)\n",
      "    10 \n",
      "    11     return df\n",
      "    12 \n",
      "    13 def calculate_stats(df):\n",
      "    14     # Calculate the median fare paid\n",
      "    15     median = df['Price'].median()\n",
      "    16     print(\"\\nMedian fare paid:\\n\", median)\n",
      "    17     return median\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[36mIn `calculate_stats`, the code uses `df['Price']`. However, the Titanic dataset usually uses the column name 'Fare' for the ticket price, not 'Price'. This discrepancy likely causes the KeyError.\n",
      "\n",
      "I will update line 15 in `pandas_dataframe_code.py` from `df['Price'].median()` to `df['Fare'].median()`.\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_wz2JYVPKEG4fU92951S1zd3I', name='rewrite', arguments={'path': 'pandas_dataframe_code.py', 'start': 15, 'end': 15, 'new_code': \"    median = df['Fare'].median()\"})\u001b[0m\n",
      "\n",
      "==================== STEP 5 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mThe file `pandas_dataframe_code.py` has been updated successfully.\n",
      "\n",
      "Diff:\n",
      "\n",
      "--- original\n",
      "+++ current\n",
      "@@ -12,6 +12,6 @@\n",
      " \n",
      " def calculate_stats(df):\n",
      "     # Calculate the median fare paid\n",
      "-    median = df['Price'].median()\n",
      "+    median = df['Fare'].median()\n",
      "     print(\"\\nMedian fare paid:\\n\", median)\n",
      "     return median\n",
      "\u001b[0m\n",
      "\u001b[36mNone\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_pt5g6Tki2qBI8XTCTekt7cYL', name='eval', arguments={})\u001b[0m\n",
      "\n",
      "==================== STEP 6 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35m============================= test session starts ==============================\n",
      "collected 1 item\n",
      "\n",
      "test.py F\n",
      "\n",
      "=========================== short test summary info ============================\n",
      "FAILED test.py::test_calculate_stats - KeyError: 'Fare'\n",
      "============================== 1 failed in 0.41s ===============================\u001b[0m\n",
      "\u001b[36mChanging 'Price' to 'Fare' did not resolve the KeyError, which means the loaded DataFrame does not have a 'Fare' column either. To diagnose further, let's inspect the actual columns of the DataFrame returned by `load_data()`. I will review a few lines from the Titanic CSV's initial rows to check the column namesâ€”this could be in the dataset URL or we can do it by looking at columns right after loading.\n",
      "\n",
      "Let's add a print statement to display the DataFrame columns right after reading the CSV in `load_data()`.\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_BbhwlQs6exgbi3vpbLaUiGKP', name='rewrite', arguments={'path': 'pandas_dataframe_code.py', 'start': 10, 'end': 10, 'new_code': \"    print('Loaded columns:', df.columns.tolist())\"})\u001b[0m\n",
      "\n",
      "==================== STEP 7 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mThe file `pandas_dataframe_code.py` has been updated successfully.\n",
      "\n",
      "Diff:\n",
      "\n",
      "--- original\n",
      "+++ current\n",
      "@@ -7,7 +7,7 @@\n",
      " \n",
      "     # Read the CSV file\n",
      "     df = pd.read_csv(url)\n",
      "-\n",
      "+    print('Loaded columns:', df.columns.tolist())\n",
      "     return df\n",
      " \n",
      " def calculate_stats(df):\n",
      "\u001b[0m\n",
      "\u001b[36mNone\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_Hq3kEkfuEWkJgYCUQobKL4WR', name='eval', arguments={})\u001b[0m\n",
      "\n",
      "==================== STEP 8 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35m============================= test session starts ==============================\n",
      "collected 1 item\n",
      "\n",
      "test.py Loaded columns: ['pclass', 'survived', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket', 'prix', 'cabin', 'embarked', 'boat', 'body', 'home.dest']\n",
      "F\n",
      "\n",
      "=========================== short test summary info ============================\n",
      "FAILED test.py::test_calculate_stats - KeyError: 'Fare'\n",
      "============================== 1 failed in 0.40s ===============================\u001b[0m\n",
      "\u001b[36mThe loaded columns include 'prix' but not 'Fare' or 'Price'. 'prix' is the French word for 'price', which implies this is the intended column for the fare/price.\n",
      "\n",
      "I will update `calculate_stats` to use `df['prix']` instead of `df['Fare']`.\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_hzm5UnlMtf1q7Xiidqo5cdNI', name='rewrite', arguments={'path': 'pandas_dataframe_code.py', 'start': 15, 'end': 15, 'new_code': \"    median = df['prix'].median()\"})\u001b[0m\n",
      "Step: 8 | Score: 1/1 (100.0%) | Reason: bug fixed\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from termcolor import colored\n",
    "\n",
    "from debug_gym.agents.history_tracker import HistoryTracker, build_history_prompt\n",
    "\n",
    "MAX_STEPS = 20  # Maximum number of steps to run the agent.\n",
    "MEMORY_SIZE = 20  # Size of the history tracker.\n",
    "TASK_NAME = \"pandas_dataframe\"\n",
    "SYSTEM_PROMPT = (\n",
    "    \"Your goal is to debug a Python program to make sure it can pass a set of test functions.\"\n",
    "    \" You have access to a set of tools, you can use them to investigate the code and propose a rewriting patch to fix the bugs.\"\n",
    "    \" Avoid rewriting the entire code, focus on the bugs only. At every step, you have to use one of the tools via function calling.\"\n",
    "    \" You can only call one tool at a time. Do not repeat your previous action unless they can provide more information.\"\n",
    "    \" You can think step by step to help you make the decision at every step, but you must be concise and avoid overthinking.\"\n",
    "    \" Output both your thinking process (if any) and the tool call in the response.\"\n",
    ")\n",
    "\n",
    "def run():\n",
    "    # We will use a history tracker to keep track of the agent's actions and observations.\n",
    "    # This will be used to build the prompt for the LLM.\n",
    "    history = HistoryTracker(MEMORY_SIZE)\n",
    "\n",
    "    # Let's reset the environment and get the initial state.\n",
    "    info = env.reset(options={\"task_name\": TASK_NAME})\n",
    "    history.step(new_info=info, llm_responses=None)  # initial state does not have response\n",
    "\n",
    "    # List the available tools that the LLM can use.\n",
    "    print(\"Available tools:\")\n",
    "    for tool in llm.define_tools(info.tools):\n",
    "        tool = tool.get(\"function\", {})\n",
    "        name = tool.get(\"name\", \"<unknown>\")\n",
    "        desc = tool.get(\"description\", \"\").split(\".\", 1)[0].strip() + \".\"\n",
    "        props = tool.get(\"parameters\", {}).get(\"properties\", {})\n",
    "        args = \", \".join(props.keys()) if props else \"\"\n",
    "        print(f\"- {name}({args}): {desc}\")\n",
    "\n",
    "    print(f\"\\nSystem prompt:\\n{colored(SYSTEM_PROMPT, 'yellow')}\")\n",
    "\n",
    "    highscore = info.score\n",
    "    for step in range(1, MAX_STEPS+1):\n",
    "        print(f\"\\n{'='*20} STEP {step} {'='*20}\")\n",
    "        highscore = max(highscore, info.score)\n",
    "        print(f\"Nb. of tests passed: {info.score:>4}/{info.max_score:<4} ({info.score/info.max_score:.1%}) [Best: {highscore}]\\n\")\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "        messages.extend(build_history_prompt(history, llm))\n",
    "\n",
    "        # Print the last observation from the environment.\n",
    "        print(colored(messages[-1][\"content\"], \"magenta\"))\n",
    "\n",
    "        # Send the prompt to the LLM and get the response.\n",
    "        llm_response = llm(messages, info.tools)\n",
    "\n",
    "        # Print the LLM response.\n",
    "        print(colored(f\"{llm_response.response}\", \"cyan\"))\n",
    "        print(colored(f\"Tool call: {llm_response.tool}\", \"cyan\", attrs=[\"bold\"]))\n",
    "\n",
    "        # Send the response to the environment and get the next state.\n",
    "        info = env.step(llm_response.tool, llm_response.response)\n",
    "        history.step(info, llm_response)\n",
    "\n",
    "        if info.terminated:\n",
    "            break\n",
    "\n",
    "    reason = \"bug fixed\" if info.resolved else \"max steps reached\"\n",
    "    print(f\"Step: {step} | Score: {info.score}/{info.max_score} ({info.score/info.max_score:.1%}) | Reason: {reason}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "history = run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec764ff",
   "metadata": {},
   "source": [
    "---\n",
    "Let's add the pdb tool to the environment and run the agent loop again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f198acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools:\n",
      "- view(path, start, end, include_line_numbers_and_breakpoints): Specify a file path to view its content.\n",
      "- rewrite(path, start, end, new_code): Rewrite the content of the specified file path, between lines [start, end], with the new code.\n",
      "- eval(): Evaluate the current code against pre-defined test cases.\n",
      "- pdb(command): An interface to the Python debugger PDB.\n",
      "\n",
      "System prompt:\n",
      "\u001b[33mYour goal is to debug a Python program to make sure it can pass a set of test functions. You have access to a set of tools, you can use them to investigate the code and propose a rewriting patch to fix the bugs. Avoid rewriting the entire code, focus on the bugs only. At every step, you have to use one of the tools via function calling. You can only call one tool at a time. Do not repeat your previous action unless they can provide more information. You can think step by step to help you make the decision at every step, but you must be concise and avoid overthinking. Output both your thinking process (if any) and the tool call in the response. Do not use rewrite for adding print statement, use the pdb tool instead.\u001b[0m\n",
      "\n",
      "==================== STEP 1 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35m============================= test session starts ==============================\n",
      "collected 1 item\n",
      "\n",
      "test.py F\n",
      "\n",
      "=========================== short test summary info ============================\n",
      "FAILED test.py::test_calculate_stats - KeyError: 'Price'\n",
      "============================== 1 failed in 0.41s ===============================\u001b[0m\n",
      "\u001b[36mThe error message indicates there's a KeyError: 'Price' likely when accessing a dictionary (perhaps a DataFrame). Let's check the code in 'test.py' first to see the test and spot how 'Price' is accessed.\n",
      "\n",
      "I'll view the first 20 lines of 'test.py' to locate the test and any references to 'Price'.\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_begvRVC6NlzNJZHmz0GGni0V', name='view', arguments={'path': 'test.py', 'start': 1, 'end': 20})\u001b[0m\n",
      "\n",
      "==================== STEP 2 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mViewing `test.py`, lines 1-8 of 8 total lines. The file is read-only.\n",
      "\n",
      "```\n",
      "     1 from pandas_dataframe_code import calculate_stats, load_data\n",
      "     2 \n",
      "     3 \n",
      "     4 def test_calculate_stats():\n",
      "     5     df = load_data()\n",
      "     6     median = calculate_stats(df)\n",
      "     7 \n",
      "     8     assert median == 14.4542\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[36mThe file `test.py` calls `load_data()` and then passes its result (a DataFrame) to `calculate_stats(df)`. The KeyError: 'Price' must be coming from one of those two functions. \n",
      "\n",
      "To find out how 'Price' is being accessed, I should next inspect the implementations of `load_data` and `calculate_stats` in `pandas_dataframe_code.py`. I will view the first 20 lines of `pandas_dataframe_code.py` to locate these definitions and see how the 'Price' key is being used.\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_4mPKpe43r2rLDNZ9Bnwt1P6N', name='view', arguments={'path': 'pandas_dataframe_code.py', 'start': 1, 'end': 20})\u001b[0m\n",
      "\n",
      "==================== STEP 3 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mViewing `pandas_dataframe_code.py`, lines 1-17 of 17 total lines.\n",
      "\n",
      "```\n",
      "     1 import pandas as pd\n",
      "     2 \n",
      "     3 \n",
      "     4 def load_data():\n",
      "     5     # URL of the Titanic dataset (CSV format)\n",
      "     6     url = \"https://gist.githubusercontent.com/chisingh/d004edf19fffe92331e153a39466d38c/raw/titanic.csv\"\n",
      "     7 \n",
      "     8     # Read the CSV file\n",
      "     9     df = pd.read_csv(url)\n",
      "    10 \n",
      "    11     return df\n",
      "    12 \n",
      "    13 def calculate_stats(df):\n",
      "    14     # Calculate the median fare paid\n",
      "    15     median = df['Price'].median()\n",
      "    16     print(\"\\nMedian fare paid:\\n\", median)\n",
      "    17     return median\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[36mThe code reads the Titanic dataset from the provided URL and then tries to access the 'Price' column (`df['Price']`). However, in the Titanic dataset the column is typically named 'Fare', not 'Price'. \n",
      "\n",
      "To fix the KeyError, I should change 'Price' to 'Fare' in the `calculate_stats` function.\n",
      "\n",
      "I'll make this correction now.\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_1zHwHnjYc4KSb86IwmshtW7W', name='rewrite', arguments={'path': 'pandas_dataframe_code.py', 'start': 15, 'end': 15, 'new_code': \"    median = df['Fare'].median()\"})\u001b[0m\n",
      "\n",
      "==================== STEP 4 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mThe file `pandas_dataframe_code.py` has been updated successfully.\n",
      "\n",
      "Diff:\n",
      "\n",
      "--- original\n",
      "+++ current\n",
      "@@ -12,6 +12,6 @@\n",
      " \n",
      " def calculate_stats(df):\n",
      "     # Calculate the median fare paid\n",
      "-    median = df['Price'].median()\n",
      "+    median = df['Fare'].median()\n",
      "     print(\"\\nMedian fare paid:\\n\", median)\n",
      "     return median\n",
      "\u001b[0m\n",
      "\u001b[36mNone\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_DmgnpmbteRLYEoSX59ZgUACQ', name='eval', arguments={})\u001b[0m\n",
      "\n",
      "==================== STEP 5 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35m============================= test session starts ==============================\n",
      "collected 1 item\n",
      "\n",
      "test.py F\n",
      "\n",
      "=========================== short test summary info ============================\n",
      "FAILED test.py::test_calculate_stats - KeyError: 'Fare'\n",
      "============================== 1 failed in 0.40s ===============================\u001b[0m\n",
      "\u001b[36mIt seems the column is still not foundâ€”KeyError: 'Fare'â€”even after changing the key name. This hints that the actual column name might be something else, or perhaps there is a whitespace issue or different capitalization.\n",
      "\n",
      "To verify the exact columns present in the DataFrame, I'll add a PDB breakpoint right before the 'Fare' access in `calculate_stats`, and inspect the column names.\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_w0cswEtDKc8bSmupynXuY3YO', name='pdb', arguments={'command': 'b pandas_dataframe_code.py:15'})\u001b[0m\n",
      "\n",
      "==================== STEP 6 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mPdb command output:\n",
      "Breakpoint 1 at /tmp/RepoEnv-pzzs7o6s/pandas_dataframe_code.py:15\n",
      "\n",
      "Current frame:\n",
      "/home/macote/src/debug-gym/.venv/lib/python3.12/site-packages/pytest/__main__.py\n",
      "\n",
      "Context around the current frame:\n",
      "  1  ->\t\"\"\"The pytest entry point.\"\"\"\n",
      "  2  \t\n",
      "  3  \tfrom __future__ import annotations\n",
      "  4  \t\n",
      "  5  \timport pytest\n",
      "  6  \t\n",
      "  7  \t\n",
      "  8  \tif __name__ == \"__main__\":\n",
      "  9  \t    raise SystemExit(pytest.console_main())\n",
      "[EOF]\n",
      "\u001b[0m\n",
      "\u001b[36mNone\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_tjblDGlTzY3QWdKEysPiCATc', name='pdb', arguments={'command': 'c'})\u001b[0m\n",
      "\n",
      "==================== STEP 7 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mPdb command output:\n",
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.12.3, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /tmp/RepoEnv-pzzs7o6s\n",
      "plugins: anyio-4.10.0\n",
      "collected 1 item                                                               \n",
      "\n",
      "test.py > /tmp/RepoEnv-pzzs7o6s/pandas_dataframe_code.py(15)calculate_stats()\n",
      "-> median = df['Fare'].median()\n",
      "\n",
      "Current frame:\n",
      "/tmp/RepoEnv-pzzs7o6s/pandas_dataframe_code.py\n",
      "\n",
      "Context around the current frame:\n",
      " 10  \t\n",
      " 11  \t    return df\n",
      " 12  \t\n",
      " 13  \tdef calculate_stats(df):\n",
      " 14  \t    # Calculate the median fare paid\n",
      " 15 B->\t    median = df['Fare'].median()\n",
      " 16  \t    print(\"\\nMedian fare paid:\\n\", median)\n",
      " 17  \t    return median\n",
      "[EOF]\n",
      "\u001b[0m\n",
      "\u001b[36mNone\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_SUK8QD60uW9Q78ORFZP5bB1d', name='pdb', arguments={'command': 'p df.columns'})\u001b[0m\n",
      "\n",
      "==================== STEP 8 ====================\n",
      "Nb. of tests passed:    0/1    (0.0%) [Best: 0]\n",
      "\n",
      "\u001b[35mPdb command output:\n",
      "Index(['pclass', 'survived', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket',\n",
      "       'prix', 'cabin', 'embarked', 'boat', 'body', 'home.dest'],\n",
      "      dtype='object')\n",
      "\n",
      "Current frame:\n",
      "/tmp/RepoEnv-pzzs7o6s/pandas_dataframe_code.py\n",
      "\n",
      "Context around the current frame:\n",
      " 10  \t\n",
      " 11  \t    return df\n",
      " 12  \t\n",
      " 13  \tdef calculate_stats(df):\n",
      " 14  \t    # Calculate the median fare paid\n",
      " 15 B->\t    median = df['Fare'].median()\n",
      " 16  \t    print(\"\\nMedian fare paid:\\n\", median)\n",
      " 17  \t    return median\n",
      "[EOF]\n",
      "\u001b[0m\n",
      "\u001b[36mThe actual column in the DataFrame that represents the fare is named 'prix', which is French for 'price'. The function should use 'prix' instead of 'Fare' or 'Price'.\n",
      "\n",
      "I will update the code to access 'prix' instead.\u001b[0m\n",
      "\u001b[1m\u001b[36mTool call: ToolCall(id='call_hmXldtZWWtPuZW80ZH78k0cX', name='rewrite', arguments={'path': 'pandas_dataframe_code.py', 'start': 15, 'end': 15, 'new_code': \"    median = df['prix'].median()\"})\u001b[0m\n",
      "Step: 8 | Score: 1/1 (100.0%) | Reason: bug fixed\n"
     ]
    }
   ],
   "source": [
    "if not env.has_tool(\"pdb\"):\n",
    "    env.add_tool(Toolbox.get_tool(\"pdb\"))\n",
    "    SYSTEM_PROMPT += (\n",
    "        \" Do not use rewrite for adding print statement, use the pdb tool instead.\"\n",
    "    )\n",
    "\n",
    "history = run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6284c",
   "metadata": {},
   "source": [
    "#### Analysis and Visualization\n",
    "\n",
    "We provide a script, `json_log_viewer.py`, to help analyze the log files (e.g., the `.jsonl` files) generated by the `debug-gym` agents.\n",
    "\n",
    "First let's save the agent's history to a `.jsonl` file. This file will contain all the interactions between the agent and the environment, including the actions taken by the agent, the observations received, and any errors encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88634798",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_output = {\n",
    "    \"problem\": TASK_NAME,\n",
    "    \"config\": {},\n",
    "    \"tools\": llm.define_tools(env.tools),\n",
    "    \"uuid\": \"N/A\",\n",
    "    \"success\": env.resolved,\n",
    "    \"log\": [history.json(i) for i in range(len(history))],\n",
    "    \"agent_type\": \"custom\",\n",
    "    \"logger\": \"N/A\",\n",
    "}\n",
    "\n",
    "with open(\"tutorial.jsonl\", \"w\") as f:\n",
    "    json.dump(jsonl_output, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e715fb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'json_log_viewer'\n",
      " * Debug mode: off\n",
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://10.209.224.183:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run the viewer.\n",
    "!python ../analysis/json_log_viewer/json_log_viewer.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "debug-gym (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
