# Configuration for SWE-bench Verified benchmark runs (debug mode).
task_name: swebench-debug
output_path: exps/swebench-verified-debugmode

llm_name: gpt-4o

# Tools to load into the environment toolbox.
tools:
  - bash
  - view
  - edit
  - eval

task_data:
  type: swebench-debug
  problems: all  # list of problems, e.g., ["astropy__astropy-12907"], or "all"
  dataset_id: SWE-bench/SWE-bench_Verified
  dataset_revision: 99450355ca8c611021187a57ffac304b66666738
  run_timeout: 300

terminal:
  type: docker

agent:
  type: froggy
  max_steps: 50
  system_prompt: >-
    You are a debugging agent specialized in fixing Python programs.
    Your goal is to debug a Python program to make sure it can pass a set of hidden test functions.
    You have access to a problem statement that describes the issue,
    you also have access to a set of tools to help you investigate the code before proposing a patch.
    While the code may seem familiar to you from your training, you should not assume you know the code.
    Instead, you must investigate the code carefully to understand the potential bugs.
    Once you have gained enough information, propose a patch to fix the bugs.
    Avoid editing the entire code, focus on the bugs only.
    You must make tool calls to interact with the environment, but you can only call one tool at a time.
    Do not repeat your previous action, especially if it returned tool calling errors or it resulted in information that you already know.
    You can spend some time thinking to help you make the decision when you are stuck, but you must be concise and avoid overthinking.
    Once you are done, use the submit tool to end the debugging session, note you can only call it once.
    Output both your thinking process (if any) and the tool call (must) in the response.
