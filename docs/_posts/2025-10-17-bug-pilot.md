---
layout: blog-post
title: "BugPilot: Complex Bug Generation For Efficient Training of SWE Agents"
date: 2025-10-17
author: "The Froggy Team"
reading_time: 10
tags: ["AI", "Debugging", "LLMs", "Software Engineering", "Training", "Agents"]
description: "We introduce FrogBoss and FrogMini, a state of the art 32B and 14B model for debugging "
authors:
  - name: "Isadora White"
    role: "Research Intern"
---



## TL;DR

> ðŸ’­ High quality bug generation is key to training the next iteration of language model based software engineering (SWE) agents
>
> ðŸ’­ Current synthetic bug pipelines (e.g. SWE-Smith) involve intentionally perturbing the code to cause issues, introducing an out-of-distribution effect from real world bug generation
>
> ðŸ’¡ We introduce a method where LLM agents try to create new features, thereby breaking existing tests unintentionally. We find through a qualitative analysis that these bugs are more similar to those generated by humans and that these bugs result in **FrogBoss** a SoTA 32B model with **54.6% pass@1 accuracy** averaged over 3 seeds on SWE-Bench-Verified.
>
>ðŸ’¡ We train **FrogMini**, a SoTA 14B model with **45.27% pass@1 accuracy** averaged over 3 seeds on SWE-Bench-Verified

![Main result bar chart]({{ 'figures/sota_png.png' | relative_url }})

Training on our more difficult bugs results in higher results than the existing 32B models on SWE-Bench-Verified. **With pass@3 we are able to outperform Claude Sonnet 4**, indicating what performance improvements could be achieved with good test-time scaling or a good test-time verifier.

## 1. Why is the synthetic generation of difficult bugs important?

> ðŸ’¡ **Synthetic bugs allow us to generate examples for training without relying on existing repositories with pull requests and GitHub issues. Using this we can easily generalize to new languages and codebases**

Many approaches to curating bug datasets rely on the expensive curation of pull requests and GitHub issues from open-source datasets. This approach has two downsides:

1. It limits the choice of repositories to those found in open-source datasets, not allowing users to create bug examples on less popular languages or repositories
2. The complexity of the bugs solved by open-source developers may not reflect the challenges involved when something goes wrong within the SWE development process

Current approaches for synthetic bug generation, such as SWE-Smith involve the injection of code perturbations into function thereby breaking the codebase intentionally. In contrast, we introduce an approach whereby an agent introduces features to a codebase (nicknamed **FeatAdd**), reflecting real-world software development practices and thereby breaks the codebase unintentionally.

![Comparison between different bug generation approaches]({{ 'figures/bug-pilot-comparison.png' | relative_url }})
*Illustration of the difference between our feature addition (FeatAdd) bug generation and intentional bug generation (e.g. SWE-Smith and an agentic approach that intentionally introduces hard bugs).*

## 2. FeatAdd & BugInstruct: Agentic Bug Generation

> ðŸ’¡ By prompting an agent to introduce a feature and thereby **unintentionally** create bugs, we can create a bug dataset that are more natural and more difficult than existing bug datasets


![An overview of our synthetic bug generation pipeline]({{'figures/bug-pilot-explanation.png' | relative_url}})
<!-- *An overview of our synthetic bug generation pipeline* -->

Our pipeline works as follows:

1. We prompt SWE-Agent to introduce a feature on one of the SWE-Smith repositories
2. We test the generated feature patch to see if it made any of the existing tests fail
3. We generate a synthetic problem statement from this
4. We use these problems as examples for supervised fine-tuning and RL fine-tuning

To reflect the process of real-world software engineering (SWE), we introduce an approach where we prompt a SWE agent to introduce a feature. By introducing this feature, the agent thereby unintentionally breaks the existing test cases. To isolate the effect of unintentional bug generation from agentic bug generation we also ablate with an agent adding bugs intentionally. In our quantitative results, we find that these intentionally generated bugs are far less useful for training and qualitatively quite dissimilar from realistic bugs.

## 3. Bug Analysis: qualitative similarity and difficulty

>ðŸ’¡ We show that our unintentionally generated bugs are more qualitatively similar to human-authored edits, through a qualitative analysis using LLMs
>
>ðŸ’¡ Our bugs are more difficult and more complicated than existing bug datasets, with Claude 4 Sonnet solving 25% fewer of our synthetically generated bugs than SWE-Smith

We hypothesize that the unintentional nature of our bug generation approach would lead to bugs being more similar to those authored by humans. Moreover, we hypothesize that these types of bugs that are more reflective of real-world software development practices would be more complicated than other approaches from scraping problems from existing repositories or intentionally perturbing codebases.

### Qualitative Analysis

We perform a qualitative analysis on our synthetic bugs, using an LLM (o3) to first summarize the bugs, and then cluster the bugs into 10 distinct categories.

We analyze five different datasets for our qualitative analysis and find that **unintentionally generated bugs more closely reflect the problems in SWE-Bench-Verified and R2E-Gym**, which are human-authored edits and are generally more diverse across different problem types.

#### Bug Datasets

**SWE-Bench-Verified:** a dataset of 500 human-authored edits curated from real-world pull requests and issues. 

**R2E-Gym:** A set of bugs curated from human-authored commits, where the authors rollback commits until the test suite breaks. 

**SWE-Smith:** A synthetic bug generation approach whereby the authors introduces small perturbations to the codebase to introduce bugs. 

**BugInstruct (ours):** An agentic bug generation approach, where the agent is instructed to ***intentionally*** break the existing tests

**FeatAdd (ours):** An agent is instructed to introduce a feature to the codebase, during which it ***unintentionally***  breaks the existing test cases.

![Qualitative analysis of bugs]({{'figures/qual_analysis.png' | relative_url}})

*We can see that unintentionally generated bugs more closely reflect the problems in SWE-Bench-Verified and R2E-Gym, which are human-authored edits and are generally more diverse across different problem types.*

### Quantitative Analysis

#### FeatAdd Bugs are more complicated

We find that our bugs are more complicated than existing bug generation techniques, illustrated by a larger number of lines and files edited in the bug patch. We compare our FeatAdd bugs to other bug datasets along the quantitative axes of the number of files modified and the number of lines changed.


![Quantitative analysis of bugs]({{'figures/quantitative_analysis.png' | relative_url}})

*A quantatitive analysis of our bugs showing that our bugs are more complex along standard metrics*

#### FeatAdd bugs are more difficult

Claude 4 Sonnet has a **25% drop in performance** on FeatAdd bugs compared to SWE-Smith, indicating that FeatAdd bugs are significantly more difficult than existing synthetic bugs (e.g. SWE-Smith). Moreover, FeatAdd bugs take on average **5 more steps to solve** than SWE-Smith tasks.

![Quantitative analysis of bugs on performance of Claude 4 Sonnet on different bug datasets]({{'figures/quantitative_analysis2.png' | relative_url}})

## 4. Results from Training on Our Tasks

> ðŸŽ¯ We fine-tune on our bugs and get SoTA results for a 32B model on SWE-Bench-Verified for 54.9% pass@1 averaged over three seeds without test-time scaling.

Training on our more difficult bugs results in higher results than the existing 32B models on SWE-Bench-Verified. With pass@3 we are able to outperform Claude Sonnet 4, indicating what performance improvements could be achieved with good test-time scaling or a good test-time verifier.

![Main result bar chart]({{ 'figures/sota_png.png' | relative_url }})

### Comparison to different training strategies

First we prepare a dataset by sampling from Claude Sonnet 4, because we find that this has significantly better performance than training on GPT-5 and GPT-4o generated trajectories.

**BaseMix**: We prepare a set of 1000 tasks from R2E-Gym and 1000 tasks from SWE-Smith and collect trajectories from these tasks.

To isolate the effect of training on FeatAdd bugs versus fine-tuning on more tasks from R2E-Gym and SWE-Smith we create new datasets of 1k tasks each from FeatAdd, SWE-Smith, BugInstruct, and R2E-Gym and test the results for further fine-tuning on these.

For the different data mixtures, we find that using our FeatAdd bugs performs **2.5% better than training on the basemix** and **2% better than training on other datasets** for further fine-tuning. Moreover, we find that finetuning with RL for further fine-tuning on the other datasets does not result in good performance. Note that the more challenging FeatAdd problems have fewer examples in the dataset, so they are thereby learning more efficiently.

![Depiction of how RL performs vs SFT and performance over BaseMix]({{ 'figures/bug-pilot-rl-vs-sft-bar-chart.png' | relative_url }})

### Reinforcement Learning

To fine-tune with reinforcement learning, we start with the model trained with the BaseMix model and fine-tune for only 25 steps with RL, where each step has 64 problems and 8 rollouts each. We find that fine-tuning with RL on other datasets is not as effective as finetuning on the FeatAdd bugs.

We do not get significant gains from RL fine-tuning on the BaseMix bugs, but we see substantial improvement when using our FeatAdd bugs for RL fine-tuning.